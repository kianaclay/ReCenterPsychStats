# Mixed Design ANOVA {#Mixed}

 [Screencasted Lecture Link](https://spu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?pid=b08debbb-948e-4f25-923a-ad8c01037e05) 
 
```{r  include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA) #keeps out the hashtags in the knits
options(scipen=999)#eliminates scientific notation
```

The focus of this lecture is mixed design ANOVA. That is, we are conducting a two-way ANOVA where one of the factors is repeated measures and one of the factors is between groups. The mixed design ANOVA is often associated with the random clinical trial (RCT) where the researcher hopes for a significant interaction effect. Specifically, the researcher hopes that the individuals who were randomly assigned to the treatment condition improve from pre-test to post-test and maintain (or continue to improve) after post-test, while the people assigned to the no-treatment control are not statistically significantly different from treatment group at pre-test, and do not improve over time.

## Navigating this Lesson

There is just over one hour of lecture.  If you work through the materials with me it would be plan for an additional two hours.

While the majority of R objects and data you will need are created within the R script that sources the chapter, occasionally there are some that cannot be created from within the R framework. Additionally, sometimes links fail.  All original materials are provided at the [Github site](https://github.com/lhbikos/ReCenterPsychStats) that hosts the book. More detailed guidelines for ways to access all these materials are provided in the OER's [introduction](#ReCintro)

### Learning Objectives

Focusing on this week's materials, make sure you can:

* Evaluate the suitability of a research design/question and dataset for conducting a mixed design ANOVA; identify alternatives if the data is not suitable.
* Test the assumptions for mixed design ANOVA.
* Conduct a mixed design ANOVA (omnibus and follow-up) in R.
* Interpret output from the mixed design ANOVA (and follow-up). 
* Prepare an APA style results section of the mixed design ANOVA output.
* Conduct a power analysis for mixed design ANOVA.

### Planning for Practice

In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty The least complex is to change the random seed and rework the problem demonstrated in the lesson. The results *should* map onto the ones obtained in the lecture. 

The second option comes from the research vignette. The Murrar and Brauer [-@murrar_entertainment-education_2018] article has three variables (attitudes toward Arabs, attitudes toward Whites, and a difference score) which are suitable for mixed design ANOVAs.  I will demonstrate a a mixed design ANOVA with the difference score. I'll leave the other two variables for opportunities for practice.

As a third option, you are welcome to use data to which you have access and is suitable for two-way ANOVA. In either case, you will be expected to:

* test the statistical assumptions
* conduct a mixed design ANOVA, including
  - omnibus test and effect size
  - report main and interaction effects
  - conduct follow-up testing of simple main effects
* write a results section to include a figure and tables

### Readings & Resources

In preparing this chapter, I drew heavily from the following resource(s). Other resources are cited (when possible, linked) in the text with complete citations in the reference list.

* Repeated Measures ANOVA in R: The Ultimate Guide. (n.d.). Datanovia. Retrieved October 19, 2020, from https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/
  - This website is an excellent guide for both one-way repeated measures and mixed design ANOVA. A great resource for both the conceptual and procedural.  This is the guide I have used for the basis of the lecture.  Working through their example would be great additional practice.
* Murrar, S., & Brauer, M. (2018). Entertainment-education effectively reduces prejudice. *Group Processes & Intergroup Relations, 21*(7), 1053–1077. https://doi.org/10.1177/1368430216682350
  - This article is the source of our research vignette. Our problem is simulated from the first of their two experiments. The authors did not conduct mixed design ANOVA. Instead, they ran independent-samples *t* tests to test the differences between the sitcom conditions for each of the three waves. This is parallel to conducting the simple-main effect analysis of condition within wave subsequent to a significant interaction.  So, this is what we do! 

### Packages

The packages used in this lesson are embedded in this code. When the hashtags are removed, the script below will (a) check to see if the following packages are installed on your computer and, if not (b) install them.
```{r }
#will install the package if not already installed
#if(!require(lavaan)){install.packages("lavaan")}
#if(!require(semPlot)){install.packages("semPlot")}
#if(!require(tidyverse)){install.packages("tidyverse")}
#if(!require(psych)){install.packages("psych")}
#if(!require(jtools)){install.packages("jtools")}
```


## Introducing Mixed Design ANOVA

Mixed design ANOVA is characterized by the following:

* at least two independent variables.  
* Termed “mixed” because 
  - one is a between-subjects factor, and 
  - one is a repeated-measures (i.e., within-subjects) factor.
* In essence, we are simultaneously conducting (hence, it’s factorial or at least 2-way) 
  - a one-way independent ANOVA and a 
  - a one-way repeated-measures ANOVA.

TEST
  
Especially when there is a significant interaction there can be numerous ways to follow up.  We will work one set of analyses:  simple main effects (condition within wave; wave within condition) and, when needed, conduct posthoc pairwise comparisons as follow-up.  Other good options include identifying a priori contrasts and conducting polynomials (not demonstrated in this lecture).

![Image of a flowchart and decision-tree for mixed design ANOVA](images/mixed/mx_workflow.jpg)

The steps in working the mixed design generally include,

1. Exploring the data/evaluating the assumptions
2. Evaluating the omnibus test
3. Follow-up to the omnibus
   - if significant interaction effect:  simple main effects and further follow-up to those
   - if significant main effect (but no significant interaction effect), identify source of significance in the main effect
   - if no significance, stop
4. Write it up with tables, figure(s)

Assumptions for the mixed design ANOVA include the following:

* The dependent variable should be continuous with no significant outliers in any cell of the design
  - Check by visualizing the data using box plots and the function *identify_outliers()* [rstatix]
* The DV should be approximately normally distributed in each cell of the design
  - Check with Shapiro-Wilk normality test *shapiro_test()* [rstatix] or visual inspection using the QQ plot (ggqqplot())*ggqqplot()[ggpubr])
* The variances of the differences between groups should be equal.  This is termed the **sphericity assumption.**  This can be checked with Mauchly's test of sphericity, which is reported automatically with *anova_test()*[rstatix].

Violation of these assumptions can take a variety of routes:

* For 2- and 3- way ANOVAs, it may be possible to transform data to be within the assumptions
* A robust ANOVA in the *WRS2* package
* If 3 or more waves/conditions and large samples, it is possible to run a multi-level, model.
* In the absence of alternatives, it may be necessary to run the mixed design with the violated assumptions, but report them.
* ....and more.  Google it.

## Research Vignette

This lesson's research vignette is from Murrar and Brauer's [-@murrar_entertainment-education_2018] article that describes the results of two studies designed to reduce prejudice against Arabs/Muslims. We are working only a portion of the first study reported in the article. Participants (*N* = 193), all who were White, were randomly assigned to one of two conditions where they watched six episodes of the sitcom [*Friends*](http://www.friends-tv.org/) or [*Little Mosque on the Prairie*](https://en.wikipedia.org/wiki/Little_Mosque_on_the_Prairie). The sitcoms and specific episodes were  selected after significant pilot testing. The selection was based on the tension between being as similar as possible, yet the intervention-oriented sitcom needed to invoke psychologial processes known to reduce prejudice. The authors felt that both series had characters that were likable and relatable and were engaged in activities of daily living. The Friends series featured characters who were predominantly White, cis-gendered, and straight. The Little Mosque series portrays the experience Western Muslims and Arabs as they live in a small Canadian town.  This study involved assessment across three waves:  baseline (before watching the assigned episodes), post1 (immediately after watching the episodes), and post2 (completed 4-6 weeks after watching the episodes).

The study used *feelings and liking thermometers*, rating their feelings and liking toward 10 different groups of people on a 0 to 100 sliding scale (with higher scores reflecting greater liking and positive feelings. For the purpose of this analysis, the ratings of attitudes toward White people and attitudes toward Arabs/Muslims were used.  A third metric was introduced by subtracting the attitudes towards Arabs/Muslims from the attitudes toward Whites. Higher scores indicated more positive attitudes toward Whites where as low scores indicated no difference in attitudes. To recap, there were three potential dependent variables, all continuously scaled:

* AttWhite:  attitudes toward White people; higher scores reflect greater liking
* AttArab:  attitudes toward Arab people; higher scores reflect greater liking
* Diff:  the difference between AttWhite and AttArab; higher scores reflect a greater liking for White people

With random assignment, nearly equal cell sizes, a condition with two levels (Friends, Little Mosque), and three waves (baseline, post1, post2), this is perfect for mixed design ANOVA.

![Image of the design for the Murrar and Brauer (2018) study](images/mixed/Murrar_design.jpg)

### Simulating the data from the journal article

Below is the code I have used to simulate the data. The simulation includes two dependent variables (AttWhite, AttArab), Wave (baseline, post1, post2), and COND (condition; Friends, Little_Mosque).  There is also a caseID (repeated three times across the three waves) and rowID (giving each observation within each case an ID). This creates the long-file, where each person has 3 rows of data representing baseline, post1, and post2.  You can use this simulation for two of the three practice suggestions.

```{r}
library(tidyverse)
set.seed(210813)#change this to any different number (and rerun the simulation) to rework the chapter problem
AttWhite<-round(c(rnorm(98,mean=76.79,sd=18.55),rnorm(95,mean=75.37,sd=18.99),rnorm(98, mean=77.47, sd=18.95), rnorm(95, mean=75.81, sd=19.29), rnorm(98, mean=77.79, sd=17.25), rnorm(95, mean=75.89, sd=19.44)),3) #sample size, M and SD for each cell; this will put it in a long file
AttWhite[AttWhite>100]<-100 #set upper bound for variable
AttWhite[AttWhite<0]<-0 #set lower bound for variable
AttArab<-round(c(rnorm(98,mean=64.11,sd=20.97),rnorm(95,mean=64.37,sd=20.03),rnorm(98, mean=64.16, sd=21.64), rnorm(95, mean=70.52, sd=18.55), rnorm(98, mean=65.29, sd=19.76), rnorm(95, mean=70.30, sd=17.98)),3)
AttArab[AttArab>100]<-100 #set upper bound for variable
AttArab[AttArab<0]<-0 #set lower bound for variable
rowID <- factor(seq(1,579))
caseID <- rep((1:193),3)
Wave <- c(rep("Baseline",193), rep("Post1", 193), rep ("Post2", 193))
COND <- c(rep("Friends", 98), rep("LittleMosque", 95), rep("Friends", 98), rep("LittleMosque", 95), rep("Friends", 98), rep("LittleMosque", 95))
Murrar_df<- data.frame(rowID, caseID, Wave, COND, AttArab, AttWhite) #groups the 3 variables into a single df:  ID#, DV, condition
#Arab_2way <- anova_test(
  #data = Murrar_df, dv = AttArab, wid = caseID, #UEdf is our df, dv is our DV, wid is the participant ID
  #between = COND, within = Wave # between is the between-subjects variable, within is the within subjects variable
#  )
#Arab_2way

#Diff<-round(c(rnorm(98,mean=12.68,sd=15.57),rnorm(95,mean=11.00,sd=17.25),rnorm(98, mean=13.31, sd=16.87), rnorm(95, mean=5.29, #sd=13.73), rnorm(98, mean=12.50, sd=16.24), rnorm(95, mean=5.60, sd=15.18)),3) #sample size, M and SD for each cell; this will #put it in a long file
```

Let's check the structure. We want 

* rowID and caseID to be unordered factors,
* Wave and COND to be ordered factors,  
* AttArab and AttWhite to be numerical

```{r}
str(Murrar_df)
```
```{r}
#make caseID a factor
Murrar_df[,'caseID'] <- as.factor(Murrar_df[,'caseID'])
#make Wave an ordered factor
Murrar_df$Wave <- factor(Murrar_df$Wave, levels = c("Baseline", "Post1", "Post2"))
#make COND an ordered factor
Murrar_df$COND <- factor(Murrar_df$COND, levels = c("Friends", "LittleMosque"))
```

Let's check the structure again.
```{r}
str(Murrar_df)
```

A key dependent variable in the Murrar and Brauer [@murrar_entertainment-education_2018] article is *attitude difference.* Specifically, the attitudes toward Arabs score was subtracted from the attitudes toward Whites scores.  Higher attitude difference indicate a greater preference for Whites.  Let's create that variable, here.

```{r}
Murrar_df$Diff <- Murrar_df$AttWhite - Murrar_df$AttArab
```

The code for the .rds file will retain the formatting of the variables, but is not easy to view outside of R. This is what I would do.
```{r}
#to save the df as an .rds (think "R object") file on your computer; it should save in the same file as the .rmd file you are working with
#saveRDS(Murrar_df, "Murrar_RDS.rds")
#bring back the simulated dat from an .rds file
#Murrar_df <- readRDS("Murrar_RDS.rds")
```

If you want to export this data as a file to your computer, remove the hashtags to save it (and re-import it) as a .csv ("Excel lite") or .rds (R object) file. This is not a necessary step.

The code for .csv will likely lose the formatting (i.e., stripping Wave and COND of their ordered factors), but it is easy to view in Excel.
```{r}
#write the simulated data  as a .csv
#write.table(Murrar_df, file="DiffCSV.csv", sep=",", col.names=TRUE, row.names=FALSE)
#bring back the simulated dat from a .csv file
#Murrar_df <- read.csv ("DiffCSV.csv", header = TRUE)
```

## Working the Mixed Design ANOVA with R packages

### Exploring data and testing assumptions

We begin the 2x3 mixed design ANOVA with a preliminary exploration of the data and testing of the assumptions. Here's where we are on the workflow:

![Image of a flowchart showing that we are on the "Evaluating assumptions" portion of the decision-tree](images/mixed/mx_Assumptions.jpg)

First, let's take a look at the descriptives overall.
```{r}
library(psych)
psych::describe(Murrar_df)
```

Our analysis will use the difference score (Diff) as the dependent variable.  Let's look at this variable in its combinations of wave and condition.
```{r }
library(psych)
psych::describeBy(Diff ~ Wave + COND, data = Murrar_df, mat=TRUE)
```
First we look at the means.  We see that the baseline scores for the Friends and Little Mosque conditions are similar. However, the post1 and post2 difference scores (i.e., difference in attitudes toward White and Arab individuals, where higher scores indicate more favorable ratings of White individuals) are higher in the Friends condition than in the Little Mosque condition.

#### Assumption of Normality

We can use this output to evaluate the distributional characteristics of the dependent variable. Recall that mixed design ANOVA assumes a normal distribution.

Our values of skew and kurtosis are well within the limits [@kline_principles_2016] of a normal distribution.

* skew: < 3; the highest skew value in our data is 0.32
* kurtosis: extreme values are between 8 and 20; the highest kurtosis value in our data is .55

The boxplot is one common way for identifying outliers.  The boxplot uses the median and the lower (25th percentile) and upper (75th percentile) quartiles.  The difference bewteen Q3 and Q1 is the *interquartile range* (IQR).  

```{r }
library(ggpubr)

CNDwiWV <- ggboxplot(
  Murrar_df, x = "Wave", y = "Diff",
  color = "COND", palette = "jco", xlab = "Assessment Wave", ylab = "Difference in Attitudes towards Whites and Arabs", 
  )
CNDwiWV
```
The distributions look relatively normal with the mean well-centered.  Given that we simulated the data from means and standard deviations, this is not terribly surprising. This boxplot also provides us a glimpse of the patterns in our data.  That is, the means are quite similar at baseline; in the post intervention waves we see greater difference scores for the Friends condition.

Let's reconfigure the data by putting the wave on the X axis.  Plotting it both ways (i.e.,swapping roles of predictor and moderator) can help us get a sense of what is happening.
```{r }
WVwiCND <- ggboxplot(
  Murrar_df, x = "COND", y = "Diff",
  color = "Wave", palette = "jco", xlab = "Treatment Condition", ylab = "Difference in Attitudes towards Whites and Arabs"
  )
WVwiCND
```
Outliers are generally identified when values fall outside these lower and upper boundaries:

* Q1 - 1.5xIQR
* Q3 + 1.5xIQR

Extreme values occur when values fall outside these boundaries:

* Q1 - 3xIQR
* Q3 + 3xIQR

Using the *rstatix* package we can look for outliers in the dependent variable, doubly grouped by our predictor variables.
```{r }
library(rstatix)
Murrar_df %>%
  group_by(Wave, COND) %>%
  identify_outliers(Diff)
```
While we have some outliers (where "is.outlier" = "TRUE"), none are extreme (where "is.outlier" = "FALSE").  We'll keep these in mind as we continue to evaluate the data.

Next we can use the *shapiro_test()* to see if any of the distributions of the dependent variable (Diff) within each wave-by-condition combinations differs significantly from a normal distribution.
```{r }
Murrar_df %>%
  group_by(Wave, COND) %>%
  shapiro_test(Diff)
```
The Shapiro Wilks test suggests that distribution in each of our cells is not significantly different than normal.  We can further visualize this with QQ plots.

```{r }
ggqqplot(Murrar_df, "Diff", ggtheme = theme_bw()) +  facet_grid(Wave ~ COND)
```
#### Homogeneity of variance assumption

Because there is a between-subjects variable, we need need to evaluate the homogeneity of variance assumption.  As before, we can use the Levene's test. Considering each of the comparisons of condition within wave, there is no instance where we violate the assumption.

```{r }
Murrar_df %>%
  group_by(Wave) %>%
  levene_test(Diff ~ COND)
```
Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline (*F* [1, 191] = 3.973, *p* = .047). However, there was no indication of assumption violation at post1 (*F* [1, 191] = 0.141, *p* = .708), and post2 (*F* [1, 191] = 0.107, *p* = .743) waves of the design.

#### Assumption of homogeneity of covariance matrices

In this multivariate sample, the Box's M test evaluates if two or more covariance matrices are homogeneous. Like other tests of assumptions, we want a non-significant test result (i.e., where *p* > .05).  Box's M has some disavantages.  One is that it has little power in small sample sizes but is also overly sensitive in large sample sizes.  So, just take a peek.

```{r }
box_m(Murrar_df[, "Diff", drop = FALSE], Murrar_df$COND)
```
None-the-less, Box's M indicated no violation of the homogeneity of covariance matrices assumption (*M* = 3.209, *p* = .073)


#### APA style writeup of assumptions

Mixed design ANOVA has a number of assumptions related to both the within-subjects and between-subjects elements. Data are expected to be normally distributed at each level of design. Visual inspection of boxplots for each wave of the design, assisted by the *identify_outliers()* function in the *rstatix* package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated some outliers, but none at the extreme level. There was no evidence of skew (all values were at or below the absolute value of 0.32) or kurtosis (all values were below the absolute value of .57; [@kline_principles_2016]). Additionally, the Shapiro-Wilk tests applied at each level of the design were non-significant. Because of the between-subjects aspect of the design, the homogeneity of variance assumption was evaluated.  Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline *F* [1, 191] = 3.973, *p* = .047). However, there was no indication of assumption violation at post1 (*F* [1, 191] = 0.141, *p* = .708), and post2 (*F* [1, 191] = 0.107, *p* = .743) waves of the design. Further, Box's M-test (*M* = 3.209, *p* = .073) indicated no violation of the homogeneity of covariance matrices. **SAVE A SPACE FOR THE SPHERICITY ASSUMPTION**

### Omnibus ANOVA

Having evaluated the assumptions (excepting sphericity) we are ready to move to the evaluation of the omnibus ANOVA. Note that the next step produces both the omnibus test as well as testing the sphericity assumption. Conceptually, evaluating the sphericity assumption precedes the omnibus; procedurally these are evaluated simultaneously.The figure also reflects decisions-related to follow-up are dependent upon the interpretation of the main and omnibus effects.

![Image of a flowchart showing that we are on the "Compute the Omnibus ANOVA" portion of the decision-tree](images/mixed/mx_omnibus.jpg)

The *rstatix* package is a wrapper for the *car* package.  Authors of *wrappers* attempt to streamline a more complex program to simplify the input needed and maximize the output produced for the typical use-cases.

Remember that a question mark placed in front of a test or package can call summons help pages for it.

```{r }
#?anova_test
```

In our package window we can see  the helper functions that result in the output we need.
```{r }
Diff_2way <- rstatix::anova_test(
  data = Murrar_df, dv = Diff, wid = caseID, #Murrar_df is our df, Diff is our df, wid is the caseID
  between = COND, within = Wave # between is the between-subjects variable, within is the within subjects variable
  )
Diff_2way
```

#### Checking the sphericity assumption
First, we check Mauchly's test for the main and interaction effects that involve the repeated measures variable.

* main effect for Wave:  *W* = .99, *p* = .369
* main effect for Wave:  *W* = .99, *p* = .369

We will be able to add this statement to our assumptions write-up:  Mauchly's test indicated no violation of the sphericity assumption for the main effect (*W* = 0.99, *p* = .369) and interaction effect  (*W* = 0.99, *p* = .369). 

If the *p* vaue associated with Mauchly's test had been less than .05, we could have used one of the two options (Greenhouse Geyser/GGe or Huynh-Feldt/HFe). In each of these an epsilon value provides an adjustment to the degrees of freedom used in the estimation of the *p* value. There is also an option to use a multivariate approach when ANOVA designs include a repeated measures factor.  

**Omnibus Results**

Results of the omnibus ANOVA indicated a significant main effect for condition (*F*[1, 191] = 13.149, *p* < .001, $\eta^{2}$ = 0.023), a non-significant main effect for wave (*F*[2, 382] = 0.273, *p* = .761, $\eta^{2}$ = 0.001), and a significant interaction effect (*F*[2, 382] = 5.008, *p* = 0.007, $\eta^{2}$ = 0.017). We note that according to Cohen et al.'s [@cohen_applied_2003] guidelines, the effect size for the interaction term is small.

In the output, the column labeled "ges" provides the value for the effect size, $\eta^{2}$. Recall that *eta-squared* is one of the most commonly used measures of effect. It refers to the proportion of variability in the dependent variable/outcome that can be explained in terms of the independent variable/predictor.  Traditional interpretive values are similar to the Pearson's *r*:

0 = no relationship
.02 = small
.13 = medium
.26 = large
1 = a perfect (one-to-one) correspondence

The effect size for our interaction effect (0.017) is small.

With a significant interaction effect, we would focus on interpreting one or both of the simple main effects.  Let's first look at the simple main effect of condition within wave option.


### Simple main effect of condition within wave

The figure reflects our path in the workflow. In the presence of a significant interaction effect we could choose from a variety of follow-up tests. 

![Image of a flowchart showing that we are on the "Simple Main Effects for Factor A within all levels of Factor B" portion of the decision-tree](images/mixed/mx_SimpleMainA.jpg)

If we take this option we follow up with 3 one-way ANOVAs.  When we look at condition within wave, our ANOVAs will look like this:

* comparison of Friends and Little Mosque within the baseline wave
* comparison of Friends and Little Mosque within the post1 wave
* comparison of Friends and Little Mosque within the post2 wave

```{r }
SimpleWave <- Murrar_df %>% #crate an object to hold the output
  group_by(Wave) %>% #this group_by function is what results in three, one-way ANOVAs for each of the waves, separately
  anova_test(dv = Diff, wid = caseID, between = COND) %>% #the between = Cond means that each level of cond will be compared
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni") #we will get both the standard and adjusted p values
SimpleWave
```
In prior lectures we have adjusted the *p* vaue against which we compare the resulting *p* value.  When we specify "bonferroni" on the  *adjust_pvalue()* command, the algorithm adjusts the reported *p* value for us. We can see the unadjusted *p* value in the "p p<.05" column and the Bonferroni adjustment in the "p.adj" column.

I think that it will be easiest for us to interpret this simple main effect as the traditional *p* < .05 and then apply the restrictions to the alpha at the next level of analysis  In this particular instance, we would have statistically significant differences somewhere between the Friends and Little Mosque conditions for both the Post (*p*  = .027) and FollowUp (*p*  = .010) waves. 

F strings:

* Pre:  *F* (1, 191) = 0.012, *p* = .914, $\eta^{2}$ = 0.000 (the effect size is zero)
* Post:  *F* (1, 191) = 17.497, *p* < .001, $\eta^{2}$ = 0.084 (approaching a moderate effect size)
* FollowUp:  *F* (1, 191) = 5.994, *p* = .015, $\eta^{2}$ = 0.030 (a small effect size)

Recall, interpretation for the eta-squared are .02 ~ small, .13 ~ medium, >.26 ~ large

Because there are only two levels (Friends, Little Mosque) within each wave (baseline, post1, post2), this simple effects analysis is complete with the three pairwise comparisons.  

As always, we have several choices about how to manage Type I error.  In a circumstance when the analysis of simple main effects (condition within wave) includes only three pairwise comparisons, we can use the LSD method [@green_using_2014]. This means that we can we can leave the alpha at 0.05. If we were to use a traditional Bonferroni, we would use $\alpha$ = .017 (.05/3). Although the more restrictive Bonferonni criteria comes close, in both cases we would still have one non-significant(baseline) and two significant (post1, post2) simple main effects. 
```{r }
.05/3
```

If we were to write up this result:

We followed the significant interaction effect with an evaluation of simple main effects of condition within wave. Because there were only three comparisons following the omnibus evaluation, we used the LSD method to control for Type I error and left the alpha at .05 [@green_using_2014]. There was a non-statistically significant difference between conditions at baseline:  *F* (1, 191) = 0.012, *p* = .914, $\eta^{2}$ = 0.000.  However other were statistically significant differences at post1 (*F* [1, 191] = 17.497, *p* < .001, $\eta^{2}$ = 0.084) and post2 (*F*[1, 191] = 5.994, *p* = .015, $\eta^{2}$ = 0.030). We note that the effect size at post1 approached a moderate size; the effect size at post2 was small.

### Simple main effect of wave within condition

Alternatively, we could evaluate the simple main effect of wave within condition. The figure reflects our path along the workflow.

![Image of a flowchart showing that we are on the "Simple Main Effects for Factor B within all levels of Factor A" portion of the decision-tree](images/mixed/mx_SimpleB.jpg)

If we conducted this alternative we would start with three one-way ANOVAs and then follow each of those with pairwise comparisons.  First, the one-way repeated measures ANOVAs:

* comparison of baseline, post1, and post2 within the Friends condition
* comparison of baseline, post1, and post2  within the Little Mosque condition

```{r }
SimpleCond <- Murrar_df %>%
  group_by(COND) %>%
  anova_test(dv = Diff, wid = caseID, within = Wave) %>%
  get_anova_table() %>%
  adjust_pvalue(method = "bonferroni")
SimpleCond
```

Below are the *F* strings for the one-way ANOVAs the followed the omnibus, mixed design, ANOVA:

* Friends:  *F* (2, 194) = 1.759, *p* = 0.175, $\eta^{2}$ = 0.012 (effect size indicates no relationship)
* Little Mosque:  *F* (2, 188) = 3.392, *p* = 0.036, $\eta^{2}$ = 0.072 (a small-to-moderate effect size)

Because each of these one-way ANOVAs has three levels, we need to follow with pairwise comparisons. However, we only need to conduct them for the Little Mosque condition. As you can see we generally work our way down to comparing chunks to each other to find the source(s) of significant differences.

```{r }
pwcWVwiGP <- Murrar_df %>%
  group_by(COND) %>%
  pairwise_t_test(
    Diff ~ Wave, paired = TRUE, detailed = TRUE,
    p.adjust.method = "bonferroni"
    ) #%>%
  #select(-df, -statistic, -p) # Remove details
pwcWVwiGP
```

At this point, we likely need to control for Type I error.  Why?  We have already conducted two one-way ANOVAs after the omnibus. Now we will conduct three more pairwise comparisons in the Little Mosque condition.  I would divide .05/3 and interpret these pairwise comparisons with an alpha of .017.

We find a significant difference between baseline and post1 (*t*[95] = 2.447, *p* = .016), but non-significant differences between baseline and post2 (*t*[95] = 1.621, *p* = .108) and post1 and post2 (*t*[95] = -1.034, *p* = .304)

If we were to write up this result:

We followed the significant interaction effect with an evaluation of simple main effects of wave within condition. There were non-significant difference within the the Friends condition (*F* [2, 194] = 1.759, *p* = 0.175, $\eta^{2}$ = 0.012). There were significant differences with an effect size indicating a small-to-moderate effect in the Little Mosque condition (*F* [2, 188] = 3.392, *p* = 0.036, $\eta^{2}$ = 0.072). We followed up the significant simple main effect for the Little Mosque condition with pairwiwse comparisons. At this level we controlled for Type I error by dividing alpha (.05) by the number of paired comparisons (3). We found a significant difference between baseline and post1 (*t*[95] = 2.447, *p* = .016), but non-significant differences between baseline and post2 (*t*[95] = 1.621, *p* = .108) and post1 and post2 (*t*[95] = -1.034, *p* = .304).

### If we only had a main effect

When there is an interaction effect, we do not interpret main effects. This is because the solution is more complicated than a main effect could explain. It is important, though, to know how to interpret a main effect.  We would do this if we had one or more significant main effects and no interaction effect.

The figure shows our place on the workflow.

![Image of a flowchart showing that we are on the "Main effects only" portion of the decision-tree](images/mixed/mx_main.jpg)

If we had not had a significant interaction, but did have a significant main effect for wave, we could have conducted pairwise comparisons for pre, post, and follow-up -- collapsing across condition.
```{r }
Murrar_df %>%
  pairwise_t_test(
    Diff ~ Wave, paired = TRUE, 
    p.adjust.method = "bonferroni"
  )
```
Ignoring condition (Friends, Little Mosque), we do not see changes across time. This is not surprising since the *F* test for the main effect was also non-significant (*F*[2, 382] = 0.273, *p* = .761, $\eta^{2}$ = 0.0014), 

If we had had a non-significant interaction effect but a significant main effect for condition, there would have been no need for further follow-up.  Why? Because there were only two levels the significant main effect already tells us there were statistically significant differences between Friends and Little Mosque (*F*[1, 191] = 13.149, *p* < .001, $\eta^{2}$ = 0.023).


### APA Style Write-up of the Results

The fancy plots below use the objects created from omnibus ANOVA and the pairwise comparisons to add results to the figure. Depending on where you are presenting your results, these may be useful. 

This first figure would pair well if you report the simple main effect of condition within wave.
```{r }

pwcWVwiGP <- pwcWVwiGP %>% add_xy_position(x = "Wave")
CNDwiWV + 
  stat_pvalue_manual(pwcWVwiGP, tip.length = 0, hide.ns = TRUE) +
  labs(
    subtitle = get_test_label(Diff_2way, detailed = TRUE),
    caption = get_pwc_label(pwcWVwiGP)
  )
```

This second figure would pair well with the results that reported the simple main effect of wave within condition.
```{r }
pwcWVwiGP <- pwcWVwiGP %>% add_xy_position(x = "Wave") #pwcWVwiGP were my pairwise comparisons for the simple effect
WVwiCND +  #WVwiCND was the boxplot before I did the ANOVA
  stat_pvalue_manual(pwcWVwiGP, tip.length = 0, hide.ns = TRUE) +
  labs(
    subtitle = get_test_label(Diff_2way, detailed = TRUE), #UE_2way was my omnibus ANOVA model
    caption = get_pwc_label(pwcWVwiGP) #and again the pairwise comparisons for the simple effect
  )
```

#### Results

We conducted a 3 X 2 mixed design ANOVA to evaluate the combined effects of condition (Friends and Little Mosque) and wave (baseline, post1, post2) a difference score that compared attitudes toward White and Arab people.  

Mixed design ANOVA has a number of assumptions related to both the within-subjects and between-subjects elements. Data are expected to be normally distributed at each level of design. Visual inspection of boxplots for each wave of the design, assisted by the *identify_outliers()* function in the *rstatix* package (which reports values above Q3 + 1.5xIQR or below Q1 - 1.5xIQR, where IQR is the interquartile range) indicated some outliers, but none at the extreme level. There was no evidence of skew (all values were at or below the absolute value of 0.32) or kurtosis (all values were below the absolute value of .57; [@kline_principles_2016]). Additionally, the Shapiro-Wilk tests applied at each level of the design were non-significant. Because of the between-subjects aspect of the design, the homogeneity of variance assumption was evaluated.  Levene's test indicated a violation of this assumption between the Friends and Little Mosque conditions at baseline *F* [1, 191] = 3.973, *p* = .047). However, there was no indication of assumption violation at post1 (*F* [1, 191] = 0.141, *p* = .708), and post2 (*F* [1, 191] = 0.107, *p* = .743) waves of the design. Further, Box's M-test (*M* = 3.209, *p* = .073) indicated no violation of the homogeneity of covariance matrices. Mauchly's test indicated no violation of the sphericity assumption for the main effect (*W* = 0.99, *p* = .369) and interaction effect  (*W* = 0.99, *p* = .369). 

Results of the omnibus ANOVA indicated a significant main effect for condition (*F*[1, 191] = 13.149, *p* < .001, $\eta^{2}$ = 0.023), a non-significant main effect for wave (*F*[2, 382] = 0.273, *p* = .761, $\eta^{2}$ = 0.001), and a significant interaction effect (*F*[2, 382] = 5.008, *p* = 0.007, $\eta^{2}$ = 0.017).

We followed the significant interaction effect with an evaluation of simple main effects of wave within condition. There were non-significant difference within the the Friends condition (*F* [2, 194] = 1.759, *p* = 0.175, $\eta^{2}$ = 0.012). There were significant differences with an effect size indicating a small-to-moderate effect in the Little Mosque condition (*F* [2, 188] = 3.392, *p* = 0.036, $\eta^{2}$ = 0.072). We followed up the significant simple main effect for the Little Mosque condition with pairwiwse comparisons. At this level we controlled for Type I error by dividing alpha (.05) by the number of paired comparisons (3). We found a significant difference between baseline and post1 (*t*[95] = 2.447, *p* = .016), but non-significant differences between baseline and post2 (*t*[95] = 1.621, *p* = .108) and post1 and post2 (*t*[95] = -1.034, *p* = .304).

As illustrated in Figure 1 difference scores were comparable at baseline. After the intervention, difference scores increased for those in the Friends condition -- indicating more favorable attitudes toward White people. In contrast, those exposed to the Little Mosque condition had difference scores that were lower. Means and standard deviations are reported in Table 1.

The following code can be used to write output to .csv files. From there it is easy(er) to manipulate them into tables for use in a write-up.
```{r }
library(MASS)
write.matrix(pwcWVwiGP, sep = ",", file = "pwcWVwiGP.csv")
#this command can also be used to export other output
write.matrix(Diff_2way$ANOVA, sep = ",", file = "Diff_2way.csv") #can get name of specific part of object by using str(object)
write.matrix(SimpleWave, sep = ",", file = "SimpleWave.csv")
write.matrix(SimpleCond, sep = ",", file = "SimpleCond.csv") 
```

#### Comparing our findings to Murrar and Brauer [-@murrar_entertainment-education_2018] 

* The authors started their primary analyses of Experiment 1 with independent *t* tests comparing the Friends and Little Mosque conditions within each of the baseline, post1, and post2 waves.  This is equivalent to our simple main effects of condition within wave that we conducted as follow-up to the significant interaction effect. It is not clear to me why they did not precede this with a mixed design ANOVA.
  - The results of the article are presented in their Table 1
  - Our results were comparable in that we found no attitude difference at baseline
  - Similar to the results in the article we found statistically significant differences (with comparable *p* values and effect sizes) at post1 and post2
* With two experiments (each with a number of associated hypotheses) in a single paper there are a large number of analyses. The tables and figures are well-chosen to represent the primary findings.
* This finding is exciting to me. Anti-racism education frequently encourages individuals to expose themselves to content authored/created by individuals from groups with marginalized identities. This finding supports that approach to prejudice reduction.

## Power in Mixed Design ANOVA

The package [*wp.rmanova*](https://webpower.psychstat.org/wiki/_media/grant/practical_statistica_interior_for_kindle.pdf) was designed for power analysis in repeated measures ANOVA.

Power analysis allows us to determine the probability of detecting an effect of a given size with a given level of confidence. Especially when we don't achieve significance, we may want to stop. 

In the *WebPower* package, we specify 6 of 7 interrelated elements; the package computes the missing element

n = sample size (number of individuals in the whole study)
ng = number of groups
nm = number of repeated measurements (i.e., waves)
f = Cohen's *f* (an effect size; we can use a conversion calculator); Cohen suggests that f values of 0.1, 0.25, and 0.4 represent small, medium, and large effect sizes, respectively.
nscor = the Greenhouse Geiser correction from our ouput; 1.0 means no correction was needed and is the package's default; < 1 means some correction was applied. 
alpha = is the probability of Type I error; we traditionally set this at .05 
power = 1 - P(Type II error) we traditionally set this at .80 (so anything less is less than what we want)
type = 0 is for between-subjects, 1 is for repeated measures, **2 is for interaction effect**. 

As in the prior lessons, we need to convert our effect size for the *interaction* to $f$ effect size (this is not the same as the *F* test). The *effectsize* package has a series of converters.  We can use the *eta2_to_f()* function. 

```{r}
library(effectsize)
eta2_to_f(0.017) #interaction effect
```

```{r  }
library(WebPower)
wp.rmanova(n=193, ng=2, nm=3, f = .1315, nscor = .99, alpha = .05, power = NULL, type = 2)
```
We are powered at .349 (we have a 35% of rejecting the null hypothesis, if it is true)

In reverse, setting *power* at .80 (the traditional value) and changing *n* to *NULL* yields a recommended sample size.    

```{r Estimate sample size}
wp.rmanova(n=NULL, ng=2, nm=3, f = .1315, nscor = .99, alpha = .05, power = .80, type = 2)
```
It thinks we need 562 participants to detect a significant interaction effect.


## Practice Problems
   
In each of these lessons I provide suggestions for practice that allow you to select one or more problems that are graded in difficulty. With each of these options I encourage you to:

* test the statistical assumptions
* conduct a two-way (minimally a 2x3), mixed design, ANOVA, including
  - omnibus test and effect size
  - report main and interaction effects
  - conduct follow-up testing of simple main effects
* write a results section to include a figure and tables


### Problem #1:  Play around with this simulation.

Copy the script for the simulation and then change (at least) one thing in the simulation to see how it impacts the results.  

* If mixed design ANOVA is new to you, perhaps you just change the number in "set.seed(210813)" from 210813 to something else. Your results should parallel those obtained in the lecture, making it easier for you to check your work as you go.
* If you are interested in power, change the sample size to something larger or smaller.
* If you are interested in variability (i.e., the homogeneity of variance assumption), perhaps you change the standard deviations in a way that violates the assumption.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|5. Describe approach for managing Type I error|    5        |_____  |   
|6. APA style results with table(s) and figure|    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          


### Problem #2:  Conduct a one-way ANOVA with a different dependent variable.

The Murrar et al. [-@murrar_entertainment-education_2018] article has three dependent variables (attitudes toward people who are Arab, attitudes toward people who are White, and the difference score).  I analyzed the difference score. Select one of the other dependent variables. If you do not get a significant interaction, play around with the simulation (changing the sample size, standard deviations, or both) until you get a significant interaction effect.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Check and, if needed, format data |      5            |_____  |           
|2. Evaluate statistical assumptions     |      5            |_____  |
|3. Conduct omnibus ANOVA (w effect size)|      5           | _____  |  
|4. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|5. Describe approach for managing Type I error |    5        |_____  |   
|6. APA style results with table(s) and figure  |    5        |_____  |       
|7. Explanation to grader                 |      5        |_____  |
|8. What part of the simulation did you change? What did you have to do to get a significant interaction effect? |     5        |_____  |
|**Totals**                               |      35       |_____  | 

### Problem #3:  Try something entirely new.

Using data for which you have permission and access (e.g.,  IRB approved data you have collected or from your lab; data you simulate from a published article; data from an open science repository; data from other chapters in this OER), complete a mixed design ANOVA. Please have at least 3 levels for one predictor and at least 2 levels for the second predictor.

Using the lecture and workflow (chart) as a guide, please work through all the steps listed in the proposed assignment/grading rubric.

|Assignment Component                    | Points Possible   | Points Earned|
|:-------------------------------------- |:----------------: |:------------:|
|1. Narrate the research vignette, describing the IV and DV | 5 |_____  |
|2. Simulate (or import) and format data               |      5            |_____  |           
|3. Evaluate statistical assumptions     |      5            |_____  |
|4. Conduct omnibus ANOVA (w effect size) |      5           | _____  |  
|5. Conduct one set of follow-up tests; narrate your choice| 5 |_____  |               
|6. Describe approach for managing Type I error |    5        |_____  |   
|7. APA style results with table(s) and figure  |    5        |_____  |       
|8 Explanation to grader                 |      5        |_____  |
|**Totals**                               |      35       |_____  |          

```{r include=FALSE}
sessionInfo()
```




